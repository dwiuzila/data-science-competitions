{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (200000, 201)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "ID_code                                                                      \n",
       "train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "           var_7   var_8  ...  var_190  var_191  var_192  var_193  var_194  \\\n",
       "ID_code                   ...                                                \n",
       "train_0  18.6266 -4.9200  ...   4.4354   3.9642   3.1364   1.6910  18.5227   \n",
       "train_1  16.5338  3.1468  ...   7.6421   7.7214   2.5837  10.9516  15.4305   \n",
       "train_2  14.6155 -4.9193  ...   2.9057   9.7905   1.6704   1.6858  21.6042   \n",
       "train_3  14.9250 -5.8609  ...   4.4666   4.7433   0.7178   1.4214  23.0347   \n",
       "train_4  19.2514  6.2654  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876   \n",
       "\n",
       "         var_195  var_196  var_197  var_198  var_199  \n",
       "ID_code                                               \n",
       "train_0  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "train_1   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "train_2   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "train_3  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "train_4  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train dataset\n",
    "\n",
    "data = pd.read_csv('train.csv', index_col=0)\n",
    "print(f'Train size:', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: (200000, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_0</th>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>8.8100</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_1</th>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>5.9739</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_2</th>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_3</th>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>7.4578</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_4</th>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>7.1437</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var_0    var_1    var_2   var_3    var_4   var_5   var_6    var_7  \\\n",
       "ID_code                                                                        \n",
       "test_0   11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493  18.2675   \n",
       "test_1    8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196  18.6316   \n",
       "test_2    5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950  20.2537   \n",
       "test_3    8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397  20.5660   \n",
       "test_4   11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595  10.6048   \n",
       "\n",
       "          var_8   var_9  ...  var_190  var_191  var_192  var_193  var_194  \\\n",
       "ID_code                  ...                                                \n",
       "test_0   2.1337  8.8100  ...  -2.1556  11.8495  -1.4300   2.4508  13.7112   \n",
       "test_1  -4.4131  5.9739  ...  10.6165   8.8349   0.9403  10.1282  15.5765   \n",
       "test_2   1.5233  8.3442  ...  -0.7484  10.9935   1.9803   2.1800  12.9813   \n",
       "test_3   3.3755  7.4578  ...   9.5702   9.0766   1.6580   3.5813  15.1874   \n",
       "test_4   2.9890  7.1437  ...   4.2259   9.1723   1.2835   3.3778  19.5542   \n",
       "\n",
       "         var_195  var_196  var_197  var_198  var_199  \n",
       "ID_code                                               \n",
       "test_0    2.4669   4.3654  10.7200  15.4722  -8.7197  \n",
       "test_1    0.4773  -1.4852   9.8714  19.1293 -20.9760  \n",
       "test_2    2.1281  -7.1086   7.0618  19.8956 -23.1794  \n",
       "test_3    3.1656   3.9567   9.2295  13.0168  -4.2108  \n",
       "test_4   -0.2860  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read test dataset\n",
    "\n",
    "etd = pd.read_csv('test.csv', index_col=0)\n",
    "print(f'Test size:', etd.shape)\n",
    "etd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some ID_codes in test dataset that are not included in scoring, probably these are fake ID_codes made by sampling real ID_codes. If this is the case, then there is a big chance that:\n",
    "\n",
    "1. if at least one of the ID_code's features is unique, then the ID_code is a real sample\n",
    "2. if none of the ID_code's features is unique, then the ID_code is a synthetic sample.\n",
    "\n",
    "credit to: https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the features.\n",
    "- orig: original 200 features\n",
    "- has_one: categorical features. Consider an element 'x' in 'orig'.\n",
    "        if x is unique in train dataset, then\n",
    "            if x is unique in train dataset only, then\n",
    "                has_one=0\n",
    "            if x is unique in train dataset and real sample from test dataset, then\n",
    "                has_one=4\n",
    "        else\n",
    "            if x appears at least another time in train dataset with target==0,\n",
    "                but doesn't appear in train dataset with target==1, then\n",
    "                has_one=1\n",
    "            else if x appears at least another time in train dataset with target==1,\n",
    "                but doesn't appear in train dataset with target==0, then\n",
    "                has_one=2\n",
    "            else (x appears at least two more time in both train dataset,\n",
    "                with target==0 and target==1), then\n",
    "                has_one=3\n",
    "- has_zero: categorical features. Consider an element 'x' in 'orig'.\n",
    "        if x appears at least another time in train dataset with target==0, then\n",
    "            has_zero=1\n",
    "        else\n",
    "            has_zero=0\n",
    "- not_u: numerical features. Consider an element 'x' in 'orig'.\n",
    "        if x is unique in train dataset, then\n",
    "            not_u=mean(train dataset)\n",
    "        else\n",
    "            not_u=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate defined features\n",
    "\n",
    "orig = [f'var_{i}' for i in range(200)]\n",
    "has_one = [f'var_{i}_has_one' for i in range(200)]\n",
    "has_zero = [f'var_{i}_has_zero' for i in range(200)]\n",
    "not_u = [f'var_{i}_not_unique' for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:13<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size after adding uniqueness features: (200000, 400)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190_u</th>\n",
       "      <th>var_191_u</th>\n",
       "      <th>var_192_u</th>\n",
       "      <th>var_193_u</th>\n",
       "      <th>var_194_u</th>\n",
       "      <th>var_195_u</th>\n",
       "      <th>var_196_u</th>\n",
       "      <th>var_197_u</th>\n",
       "      <th>var_198_u</th>\n",
       "      <th>var_199_u</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_0</th>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>8.8100</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_1</th>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>5.9739</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_2</th>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_3</th>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>7.4578</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_4</th>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>7.1437</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var_0    var_1    var_2   var_3    var_4   var_5   var_6    var_7  \\\n",
       "ID_code                                                                        \n",
       "test_0   11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493  18.2675   \n",
       "test_1    8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196  18.6316   \n",
       "test_2    5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950  20.2537   \n",
       "test_3    8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397  20.5660   \n",
       "test_4   11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595  10.6048   \n",
       "\n",
       "          var_8   var_9  ...  var_190_u  var_191_u  var_192_u  var_193_u  \\\n",
       "ID_code                  ...                                               \n",
       "test_0   2.1337  8.8100  ...      False      False      False      False   \n",
       "test_1  -4.4131  5.9739  ...      False      False      False      False   \n",
       "test_2   1.5233  8.3442  ...      False      False      False      False   \n",
       "test_3   3.3755  7.4578  ...      False      False      False      False   \n",
       "test_4   2.9890  7.1437  ...      False      False      False      False   \n",
       "\n",
       "         var_194_u  var_195_u  var_196_u  var_197_u  var_198_u  var_199_u  \n",
       "ID_code                                                                    \n",
       "test_0       False      False      False      False      False      False  \n",
       "test_1       False      False      False      False      False      False  \n",
       "test_2       False      False      False      False      False      False  \n",
       "test_3       False      False       True       True      False      False  \n",
       "test_4       False      False      False      False      False      False  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each original feature 'f' in test dataset, \n",
    "# make another binary feature that says \n",
    "# whether each ID_code's element in feature 'f' is unique\n",
    "\n",
    "# let's call these features as 'uniqueness features'\n",
    "\n",
    "for f in tqdm(orig):\n",
    "    unique_v = etd[f].value_counts()\n",
    "    unique_v = unique_v.index[unique_v == 1]\n",
    "    etd[f + '_u'] = etd[f].isin(unique_v)\n",
    "    \n",
    "print(f'Test size after adding uniqueness features:', etd.shape)\n",
    "etd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real ID_codes: 100000\n"
     ]
    }
   ],
   "source": [
    "# make a feature 'has_unique' in test dataset,\n",
    "# stating whether each ID_code has a 'True' in 'uniqueness features'\n",
    "\n",
    "etd['has_unique'] = etd[[f + '_u' for f in orig]].any(axis=1)\n",
    "print(f'Number of real ID_codes:', etd['has_unique'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose ID_codes from test dataset that have unique values corresponding to a feature,\n",
    "# then take only original features of those ID_codes\n",
    "\n",
    "# let's call this new dataframe as 'real_samples'\n",
    "\n",
    "real_samples = etd.loc[etd['has_unique'], orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All real dataset size: (300000, 201)\n"
     ]
    }
   ],
   "source": [
    "# concatenate train dataset with 'real_samples' to make 'ref'\n",
    "\n",
    "ref = pd.concat([data, real_samples], axis=0, sort=True)\n",
    "print(f'All real dataset size:', ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [05:03<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# for each original feature 'f' in train dataset\n",
    "\n",
    "for f in tqdm(orig):\n",
    "\n",
    "    # initialize new features\n",
    "    data[f + '_has_one'] = 0\n",
    "    data[f + '_has_zero'] = 0\n",
    "    \n",
    "    # f_1_1: for each ID_code with target==1, select its element of feature 'f' with non-unique occurences in 'f'\n",
    "    # f_0_1: for each ID_code with target==1, select its element of feature 'f'\n",
    "    f_1 = data.loc[data['target'] == 1, f].value_counts()\n",
    "    f_1_1 = set(f_1.index[f_1 > 1])\n",
    "    f_0_1 = set(f_1.index[f_1 > 0])\n",
    "\n",
    "    # f_0_0: for each ID_code with target==0, select its element of feature 'f' with non-unique occurences in 'f'\n",
    "    # f_1_0: for each ID_code with target==0, select its element of feature 'f'\n",
    "    f_0 = data.loc[data['target'] == 0, f].value_counts()\n",
    "    f_0_0 = set(f_0.index[f_0 > 1])\n",
    "    f_1_0 = set(f_0.index[f_0 > 0])\n",
    "    \n",
    "    # for an element in feature 'f', if the same element occurs in other ID_code that has target==1, then has_one=1\n",
    "    # otherwise, has_one=0\n",
    "    data.loc[data['target'] == 1, f + '_has_one'] = data.loc[data['target'] == 1, f].isin(f_1_1).astype(int)\n",
    "    data.loc[data['target'] == 0, f + '_has_one'] = data.loc[data['target'] == 0, f].isin(f_0_1).astype(int)\n",
    "\n",
    "    # for an element in feature 'f', if the same element occurs in other ID_code that has target==0, then has_zero=1\n",
    "    # otherwise, has_zero=0\n",
    "    data.loc[data['target'] == 1, f + '_has_zero'] = data.loc[data['target'] == 1, f].isin(f_1_0).astype(int)\n",
    "    data.loc[data['target'] == 0, f + '_has_zero'] = data.loc[data['target'] == 0, f].isin(f_0_0).astype(int)\n",
    "\n",
    "# revise the definition of has_one as explained above\n",
    "data.loc[:, has_one] = 2*data.loc[:, has_one].values + data.loc[:, has_zero].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [03:47<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# analog for test dataset\n",
    "\n",
    "for f in tqdm(orig):\n",
    "    etd[f + '_has_one'] = 0\n",
    "    etd[f + '_has_zero'] = 0\n",
    "    f_1 = data.loc[data['target'] == 1, f].unique()\n",
    "    f_0 = data.loc[data['target'] == 0, f].unique()\n",
    "    etd.loc[:, f + '_has_one'] = etd[f].isin(f_1).astype(int)\n",
    "    etd.loc[:, f + '_has_zero'] = etd[f].isin(f_0).astype(int)\n",
    "    \n",
    "etd.loc[:, has_one] = 2*etd.loc[:, has_one].values + etd.loc[:, has_zero].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [10:10<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# for each original feature 'f' in ref dataset\n",
    "\n",
    "for f in tqdm(orig):\n",
    "    \n",
    "    # obtain elements with non-unique occurences in 'f'\n",
    "    v = ref[f].value_counts()\n",
    "    non_unique_v = v.index[v != 1]\n",
    "    \n",
    "    # replace the elements with unique occurences in 'f' in train dataset\n",
    "    # by the average of all elements of 'f' in train dataset,\n",
    "    # then assign this change to a new feature 'not_unique'\n",
    "    m_trd = data[f].isin(non_unique_v)\n",
    "    data[f + '_not_unique'] = m_trd  * data[f] + (~m_trd) * data[f].mean()\n",
    "    \n",
    "    # replace the elements with unique occurences in 'f' in test dataset\n",
    "    # by the average of all elements of 'f' in train dataset\n",
    "    # then assign this change to a new feature 'not_unique'\n",
    "    m_etd = etd[f].isin(non_unique_v)\n",
    "    etd[f + '_not_unique'] = m_etd  * etd[f] + (~m_etd) * data[f].mean()\n",
    "    \n",
    "    # replace elements of has_one features from has_one=0 to has_one=4\n",
    "    # if those elements are also unique in 'f' of ref\n",
    "    data.loc[~m_trd, f + '_has_one'] = 4\n",
    "    etd.loc[~m_etd, f + '_has_one'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    126381\n",
      "3     27409\n",
      "4     23064\n",
      "0     17291\n",
      "2      5855\n",
      "Name: var_0_has_one, dtype: int64\n",
      "1    153790\n",
      "0     46210\n",
      "Name: var_0_has_zero, dtype: int64\n",
      "1    125826\n",
      "3     28183\n",
      "4     23096\n",
      "0     17021\n",
      "2      5874\n",
      "Name: var_0_has_one, dtype: int64\n",
      "1    154009\n",
      "0     45991\n",
      "Name: var_0_has_zero, dtype: int64\n",
      "(200000, 801)\n",
      "(200000, 1001)\n"
     ]
    }
   ],
   "source": [
    "# sanity check the data\n",
    "\n",
    "print(data['var_0_has_one'].value_counts())\n",
    "print(data['var_0_has_zero'].value_counts())\n",
    "print(etd['var_0_has_one'].value_counts())\n",
    "print(etd['var_0_has_zero'].value_counts())\n",
    "print(data.shape)\n",
    "print(etd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_0_has_zero</th>\n",
       "      <th>var_0_has_one</th>\n",
       "      <th>var_0_not_unique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.9255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.5006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.6093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.8369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target    var_0  var_0_has_zero  var_0_has_one  var_0_not_unique\n",
       "ID_code                                                                  \n",
       "train_0       0   8.9255               1              1            8.9255\n",
       "train_1       0  11.5006               1              1           11.5006\n",
       "train_2       0   8.6093               1              1            8.6093\n",
       "train_3       0  11.0604               1              1           11.0604\n",
       "train_4       0   9.8369               1              1            9.8369"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:, ['target', 'var_0', 'var_0_has_zero', 'var_0_has_one', 'var_0_not_unique']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_0_u</th>\n",
       "      <th>has_unique</th>\n",
       "      <th>var_0_has_zero</th>\n",
       "      <th>var_0_has_one</th>\n",
       "      <th>var_0_not_unique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_0</th>\n",
       "      <td>11.0656</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_1</th>\n",
       "      <td>8.5304</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8.5304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_2</th>\n",
       "      <td>5.4827</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.4827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_3</th>\n",
       "      <td>8.5374</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_4</th>\n",
       "      <td>11.7058</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           var_0  var_0_u  has_unique  var_0_has_zero  var_0_has_one  \\\n",
       "ID_code                                                                \n",
       "test_0   11.0656    False       False               1              1   \n",
       "test_1    8.5304    False       False               1              3   \n",
       "test_2    5.4827    False       False               1              1   \n",
       "test_3    8.5374     True        True               1              1   \n",
       "test_4   11.7058    False       False               1              1   \n",
       "\n",
       "         var_0_not_unique  \n",
       "ID_code                    \n",
       "test_0            11.0656  \n",
       "test_1             8.5304  \n",
       "test_2             5.4827  \n",
       "test_3             8.5374  \n",
       "test_4            11.7058  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etd.loc[:, ['var_0', 'var_0_u', 'has_unique', 'var_0_has_zero', 'var_0_has_one', 'var_0_not_unique']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets to local\n",
    "\n",
    "pd.DataFrame.to_csv(data.reset_index(), '921_data.csv')\n",
    "pd.DataFrame.to_csv(etd.reset_index(), '921_etd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import fastai\n",
    "from fastai.tabular import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger\n",
    "\n",
    "def get_logger():\n",
    "    FORMAT = '[%(levelname)s]%(asctime)s:%(name)s:%(message)s'\n",
    "    logging.basicConfig(format=FORMAT)\n",
    "    logger = logging.getLogger('main')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "    \n",
    "logger = get_logger()\n",
    "\n",
    "def auroc_score(input, target):\n",
    "    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\n",
    "    return roc_auc_score(target, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to calculate AUC at the end of each epoch\n",
    "\n",
    "class AUROC(Callback):\n",
    "    _order = -20 # needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs):\n",
    "        self.learn = learn\n",
    "        self.output = []\n",
    "        self.target = []\n",
    "        \n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, train, **kwargs):\n",
    "        if not train:\n",
    "            self.output.append(last_output)\n",
    "            self.target.append(last_target)\n",
    "                \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if len(self.output) > 0:\n",
    "            output = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            metric = auroc_score(preds, target)\n",
    "            return add_metrics(last_metrics, [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback that do the shuffle augmentation        \n",
    "\n",
    "class AugShuffCallback(LearnerCallback):\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        if not train: return\n",
    "        m_pos = last_target==1\n",
    "        m_neg = last_target==0\n",
    "        \n",
    "        pos_cat = last_input[0][m_pos]\n",
    "        pos_cont = last_input[1][m_pos]\n",
    "        \n",
    "        neg_cat = last_input[0][m_neg]\n",
    "        neg_cont = last_input[1][m_neg]\n",
    "        \n",
    "        for f in range(200):\n",
    "            shuffle_pos = torch.randperm(pos_cat.size(0)).to(last_input[0].device)\n",
    "            pos_cat[:,f] = pos_cat[shuffle_pos,f]\n",
    "            pos_cont[:,f] = pos_cont[shuffle_pos, f]\n",
    "            pos_cont[:,f+200] = pos_cont[shuffle_pos, f+200]\n",
    "            \n",
    "            shuffle_neg = torch.randperm(neg_cat.size(0)).to(last_input[0].device)\n",
    "            neg_cat[:,f] = neg_cat[shuffle_neg,f]\n",
    "            neg_cont[:, f] = neg_cont[shuffle_neg, f]\n",
    "            neg_cont[:,f+200] = neg_cont[shuffle_neg, f+200]\n",
    "        \n",
    "        new_input = [torch.cat([pos_cat, neg_cat]), torch.cat([pos_cont, neg_cont])]\n",
    "        new_target = torch.cat([last_target[m_pos], last_target[m_neg]])\n",
    "        \n",
    "        return {'last_input': new_input, 'last_target': new_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a longer version of the random sampler : each samples is given \"mult\" times.\n",
    "\n",
    "class LongerRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, replacement=False, num_samples=None, mult=3):\n",
    "        self.data_source = data_source\n",
    "        self.replacement = replacement\n",
    "        self.num_samples = num_samples\n",
    "        self.mult = mult\n",
    "\n",
    "        if self.num_samples is not None and replacement is False:\n",
    "            raise ValueError(\"With replacement=False, num_samples should not be specified, \"\n",
    "                             \"since a random permute will be performed.\")\n",
    "\n",
    "        if self.num_samples is None:\n",
    "            self.num_samples = len(self.data_source) * self.mult\n",
    "\n",
    "        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
    "            raise ValueError(\"num_samples should be a positive integeral \"\n",
    "                             \"value, but got num_samples={}\".format(self.num_samples))\n",
    "        if not isinstance(self.replacement, bool):\n",
    "            raise ValueError(\"replacement should be a boolean value, but got \"\n",
    "                             \"replacement={}\".format(self.replacement))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = len(self.data_source)\n",
    "        if self.replacement:\n",
    "            return iter(torch.randint(high=n, size=(self.num_samples*self.mult,), dtype=torch.int64).tolist())\n",
    "        return iter(torch.randperm(n).tolist()*self.mult)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)*self.mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the NN structure, starting from fast.ai TabularModel\n",
    "\n",
    "class my_TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
    "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False, \n",
    "                 cont_emb=2, cont_emb_notu=2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # continuous embedding NN for raw features\n",
    "        self.cont_emb = cont_emb[1]\n",
    "        self.cont_emb_l = torch.nn.Linear(1 + 2, cont_emb[0])\n",
    "        self.cont_emb_l2 = torch.nn.Linear(cont_emb[0], cont_emb[1])\n",
    "        \n",
    "        # continuous embedding NN for \"not unique\" features\", cf #1 solution post\n",
    "        self.cont_emb_notu_l = torch.nn.Linear(1 + 2, cont_emb_notu[0])\n",
    "        self.cont_emb_notu_l2 = torch.nn.Linear(cont_emb_notu[0], cont_emb_notu[1])\n",
    "        self.cont_emb_notu = cont_emb_notu[1]\n",
    "            \n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        \n",
    "        # embedding for \"has one\" categorical features, cf #1 solution post\n",
    "        self.embeds = embedding(emb_szs[0][0], emb_szs[0][1])\n",
    "        \n",
    "        # at first we included information about the variable being processed (to extract feature importance)\n",
    "        # it works better using a constant feat (kind of intercept)\n",
    "        self.embeds_feat = embedding(201, 2)\n",
    "        self.embeds_feat_w = embedding(201, 2)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        \n",
    "        n_emb = self.embeds.embedding_dim\n",
    "        n_emb_feat = self.embeds_feat.embedding_dim\n",
    "        n_emb_feat_w = self.embeds_feat_w.embedding_dim\n",
    "        \n",
    "        self.n_emb, self.n_emb_feat, self.n_emb_feat_w, self.n_cont,self.y_range = n_emb, n_emb_feat, n_emb_feat_w, n_cont, y_range\n",
    "        \n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.seq = nn.Sequential()\n",
    "        \n",
    "        # input size for the NN that predicts weights\n",
    "        inp_w = self.n_emb + self.n_emb_feat_w + self.cont_emb + self.cont_emb_notu\n",
    "        # input size for the final NN that predicts output\n",
    "        inp_x = self.n_emb + self.cont_emb + self.cont_emb_notu\n",
    "        \n",
    "        # NN that predicts the weights\n",
    "        self.weight = nn.Linear(inp_w, 5)\n",
    "        self.weight2 = nn.Linear(5,1)\n",
    "        \n",
    "        mom = 0.1\n",
    "        self.bn_cat = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_feat_emb = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_feat_w = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_raw = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_notu = nn.BatchNorm1d(200, momentum=mom)\n",
    "        self.bn_w = nn.BatchNorm1d(inp_w, momentum=mom)\n",
    "        self.bn = nn.BatchNorm1d(inp_x, momentum=mom)\n",
    "        \n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [self.n_emb + self.cont_emb_notu + self.cont_emb] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
    "        b_size = x_cont.size(0)\n",
    "        \n",
    "        # embedding of has one feat\n",
    "        x = [self.embeds(x_cat[:,i]) for i in range(200)]\n",
    "        x = torch.stack(x, dim=1)\n",
    "        \n",
    "        # embedding of intercept. It was embedding of feature id before\n",
    "        x_feat_emb = self.embeds_feat(x_cat[:,200])\n",
    "        x_feat_emb = torch.stack([x_feat_emb]*200, 1)\n",
    "        x_feat_emb = self.bn_feat_emb(x_feat_emb)\n",
    "        x_feat_w = self.embeds_feat_w(x_cat[:,200])\n",
    "        x_feat_w = torch.stack([x_feat_w]*200, 1)\n",
    "        \n",
    "        # \"continuous embedding\" of raw features\n",
    "        x_cont_raw = x_cont[:,:200].contiguous().view(-1, 1)\n",
    "        x_cont_raw = torch.cat([x_cont_raw, x_feat_emb.view(-1, self.n_emb_feat)], 1)\n",
    "        x_cont_raw = F.relu(self.cont_emb_l(x_cont_raw))\n",
    "        x_cont_raw = self.cont_emb_l2(x_cont_raw)\n",
    "        x_cont_raw = x_cont_raw.view(b_size, 200, self.cont_emb)\n",
    "        \n",
    "        # \"continuous embedding\" of not unique features\n",
    "        x_cont_notu = x_cont[:,200:].contiguous().view(-1, 1)\n",
    "        x_cont_notu = torch.cat([x_cont_notu, x_feat_emb.view(-1,self.n_emb_feat)], 1)\n",
    "        x_cont_notu = F.relu(self.cont_emb_notu_l(x_cont_notu))\n",
    "        x_cont_notu = self.cont_emb_notu_l2(x_cont_notu)\n",
    "        x_cont_notu = x_cont_notu.view(b_size, 200, self.cont_emb_notu)\n",
    "\n",
    "        x_cont_notu = self.bn_notu(x_cont_notu)\n",
    "        x = self.bn_cat(x)\n",
    "        x_cont_raw = self.bn_raw(x_cont_raw)\n",
    "\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont_raw = self.emb_drop(x_cont_raw)\n",
    "        x_cont_notu = self.emb_drop(x_cont_notu)\n",
    "        x_feat_w = self.bn_feat_w(x_feat_w)\n",
    "        \n",
    "        # predict a weight for each of the previous embeddings\n",
    "        x_w = torch.cat([x.view(-1,self.n_emb),\n",
    "                         x_feat_w.view(-1,self.n_emb_feat_w),\n",
    "                         x_cont_raw.view(-1, self.cont_emb), \n",
    "                         x_cont_notu.view(-1, self.cont_emb_notu)], 1)\n",
    "\n",
    "        x_w = self.bn_w(x_w)\n",
    "\n",
    "        w = F.relu(self.weight(x_w))\n",
    "        w = self.weight2(w).view(b_size, -1)\n",
    "        w = torch.nn.functional.softmax(w, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # weighted average of the differents embeddings using weights given by NN\n",
    "        x = (w * x).sum(dim=1)\n",
    "        x_cont_raw = (w * x_cont_raw).sum(dim=1)\n",
    "        x_cont_notu = (w * x_cont_notu).sum(dim=1)\n",
    "        \n",
    "        # Use NN on the weighted average to predict final output\n",
    "        x = torch.cat([x, x_cont_raw, x_cont_notu], 1) if self.n_emb != 0 else x_cont\n",
    "        x = self.bn(x)\n",
    "            \n",
    "        x = self.seq(x)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-11-09 09:18:31,810:main:Input data\n"
     ]
    }
   ],
   "source": [
    "# load datasets and set up features\n",
    "\n",
    "logger.info('Input data')\n",
    "\n",
    "data = pd.read_csv('921_data.csv')\n",
    "data = data.set_index('ID_code')\n",
    "\n",
    "etd = pd.read_csv('921_etd.csv')\n",
    "etd = etd.set_index('ID_code')\n",
    "\n",
    "has_one = [f'var_{i}_has_one' for i in range(200)]\n",
    "orig = [f'var_{i}' for i in range(200)]\n",
    "not_u = [f'var_{i}_not_unique' for i in range(200)]\n",
    "cont_vars = orig + not_u\n",
    "cat_vars = has_one\n",
    "target = 'target'\n",
    "path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-11-09 09:20:21,851:main:cat treatment\n"
     ]
    }
   ],
   "source": [
    "# categorical features treatment\n",
    "\n",
    "logger.info('cat treatment')\n",
    "\n",
    "for f in cat_vars:\n",
    "    data[f] = data[f].astype('category').cat.as_ordered()\n",
    "    etd[f] = pd.Categorical(etd[f], categories=data[f].cat.categories, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant feature to replace feature index information\n",
    "\n",
    "feat = ['intercept']\n",
    "data['intercept'] = 1\n",
    "data['intercept'] = data['intercept'].astype('category')\n",
    "etd['intercept'] = 1\n",
    "etd['intercept'] = etd['intercept'].astype('category')\n",
    "    \n",
    "cat_vars += feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "\n",
    "ss = StandardScaler()\n",
    "data[cont_vars] = ss.fit_transform(data[cont_vars].values)\n",
    "etd[cont_vars] = ss.transform(etd[cont_vars].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: (200000, 602)\n",
      "Test dataset size: (200000, 601)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection\n",
    "\n",
    "data = data[cont_vars + cat_vars + ['target']]\n",
    "etd = etd[cont_vars + cat_vars]\n",
    "data[target] = data[target].astype('int')\n",
    "print(f'Train dataset size:', data.shape)\n",
    "print(f'Test dataset size:', etd.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for validation\n",
    "\n",
    "fold_seed = 42\n",
    "ss = StratifiedKFold(n_splits=5, random_state=fold_seed, shuffle=True)\n",
    "\n",
    "folds = []\n",
    "for num, (train,test) in enumerate(ss.split(data[target], data[target])):\n",
    "    folds.append([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "\n",
    "layers=[32]\n",
    "ps=0.2\n",
    "emb_drop=0.08\n",
    "cont_emb=(50,10)\n",
    "cont_emb_notu=(50,10)\n",
    "emb_szs = [[6,12]]\n",
    "use_bn = True\n",
    "joined=False\n",
    "seeds = [42]\n",
    "\n",
    "results = []\n",
    "sub_preds = pd.DataFrame(columns=range(10), index=etd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & predict\n",
    "\n",
    "for num_fold, (train, test) in enumerate(folds):\n",
    "    procs=[]\n",
    "    df = (TabularList.from_df(data, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs)\n",
    "              .split_by_idx(test)\n",
    "              .label_from_df(cols=target)\n",
    "              .add_test(TabularList.from_df(etd, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs))\n",
    "              .databunch(num_workers=0, bs=1024))\n",
    "            \n",
    "    df.dls[0].dl = df.dls[0].new(sampler=LongerRandomSampler(data_source=df.train_ds, mult=2), shuffle=False).dl\n",
    "    for num_seed, seed in enumerate(seeds):\n",
    "        logger.info(f'Model {num_fold} seed {num_seed}')\n",
    "        set_seed(seed)\n",
    "        model = my_TabularModel(emb_szs, len(df.cont_names), out_sz=df.c, layers=layers, ps=ps, emb_drop=emb_drop,\n",
    "                                y_range=None, use_bn=use_bn, cont_emb=cont_emb, cont_emb_notu=cont_emb_notu)\n",
    "\n",
    "        learn = Learner(df, model, metrics=None, callback_fns=AUROC, wd=0.1)\n",
    "        learn.fit_one_cycle(15, \n",
    "                            max_lr=1e-2, \n",
    "                            callbacks=[SaveModelCallback(learn, \n",
    "                                                         every='improvement', \n",
    "                                                         monitor='AUROC', \n",
    "                                                         name=f'fold{fold_seed}_{num_fold}_seed_{seed}'), \n",
    "                                       AugShuffCallback(learn)])\n",
    "        pred, _ = learn.get_preds()\n",
    "        pred = pred[:,1]\n",
    "        \n",
    "        pred_test, _ = learn.get_preds(DatasetType.Test)\n",
    "        pred_test = pred_test[:,1]\n",
    "        \n",
    "        sub_preds.loc[:, num_fold] = pred_test\n",
    "        # results.append(np.max(learn.recorder.metrics))\n",
    "        # logger.info('result ' + str(results[-1]))\n",
    "        \n",
    "        np.save(f'oof_fold{fold_seed}_{num_fold}_seed_{seed}.npy', pred)\n",
    "        np.save(f'test_fold{fold_seed}_{num_fold}_seed_{seed}.npy', pred_test)\n",
    "        \n",
    "        del learn, pred, model, pred_test; gc.collect()\n",
    "    del df; gc.collect()\n",
    "# print(results)\n",
    "# print(np.mean(results))\n",
    "\n",
    "sub_preds[target] = sub_preds.rank().mean(axis=1)\n",
    "sub_preds[[target]].to_csv('submission_NN_wo_pseudo_seed42.csv', index_label='ID_code')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
